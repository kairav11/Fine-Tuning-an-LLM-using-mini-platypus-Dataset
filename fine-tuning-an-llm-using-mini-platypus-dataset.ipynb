{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine Tuning an LLM using mini-platypus Dataset","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q accelerate peft bitsandbytes transformers trl datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:39.642812Z","iopub.execute_input":"2025-07-17T22:44:39.643864Z","iopub.status.idle":"2025-07-17T22:44:48.965027Z","shell.execute_reply.started":"2025-07-17T22:44:39.643832Z","shell.execute_reply":"2025-07-17T22:44:48.963841Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Imports\nimport os\nimport torch\nimport warnings\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    TrainingArguments, \n    pipeline\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nwarnings.filterwarnings('ignore')\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:48.967286Z","iopub.execute_input":"2025-07-17T22:44:48.967587Z","iopub.status.idle":"2025-07-17T22:44:49.024207Z","shell.execute_reply.started":"2025-07-17T22:44:48.967564Z","shell.execute_reply":"2025-07-17T22:44:49.023545Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Using a small model suitable for Kaggle's Free GPU\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ndataset_name = \"mlabonne/mini-platypus\"\nnew_model = \"tinyllama-mini-platypus\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:49.025500Z","iopub.execute_input":"2025-07-17T22:44:49.025757Z","iopub.status.idle":"2025-07-17T22:44:49.029499Z","shell.execute_reply.started":"2025-07-17T22:44:49.025737Z","shell.execute_reply":"2025-07-17T22:44:49.028672Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# LoRA and quantization settings\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:49.031740Z","iopub.execute_input":"2025-07-17T22:44:49.032347Z","iopub.status.idle":"2025-07-17T22:44:49.041307Z","shell.execute_reply.started":"2025-07-17T22:44:49.032325Z","shell.execute_reply":"2025-07-17T22:44:49.040463Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Training configuration\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:49.042481Z","iopub.execute_input":"2025-07-17T22:44:49.042832Z","iopub.status.idle":"2025-07-17T22:44:49.054729Z","shell.execute_reply.started":"2025-07-17T22:44:49.042771Z","shell.execute_reply":"2025-07-17T22:44:49.054021Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Load dataset\ndataset_name = \"mlabonne/mini-platypus\"\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.map(lambda x: {\"text\": x[\"instruction\"][:256]})  \ndataset = dataset.select(range(10)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:49.055644Z","iopub.execute_input":"2025-07-17T22:44:49.055901Z","iopub.status.idle":"2025-07-17T22:44:50.228613Z","shell.execute_reply.started":"2025-07-17T22:44:49.055881Z","shell.execute_reply":"2025-07-17T22:44:50.227951Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Setup quantization config\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n   load_in_4bit=use_4bit,\n   bnb_4bit_quant_type=bnb_4bit_quant_type,\n   bnb_4bit_compute_dtype=compute_dtype,\n   bnb_4bit_use_double_quant=use_nested_quant,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:50.229539Z","iopub.execute_input":"2025-07-17T22:44:50.229760Z","iopub.status.idle":"2025-07-17T22:44:50.235860Z","shell.execute_reply.started":"2025-07-17T22:44:50.229742Z","shell.execute_reply":"2025-07-17T22:44:50.234961Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# GPU check\nif compute_dtype == torch.float16 and use_4bit:\n   major, _ = torch.cuda.get_device_capability()\n   if major >= 8:\n       print(\"=\" * 80)\n       print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n       print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:50.237164Z","iopub.execute_input":"2025-07-17T22:44:50.237494Z","iopub.status.idle":"2025-07-17T22:44:50.245490Z","shell.execute_reply.started":"2025-07-17T22:44:50.237456Z","shell.execute_reply":"2025-07-17T22:44:50.244839Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n   model_name,\n   quantization_config=bnb_config,\n   \n   device_map={\"\": 0},\n   \n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:50.246376Z","iopub.execute_input":"2025-07-17T22:44:50.246610Z","iopub.status.idle":"2025-07-17T22:44:52.846993Z","shell.execute_reply.started":"2025-07-17T22:44:50.246592Z","shell.execute_reply":"2025-07-17T22:44:52.846256Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:52.850554Z","iopub.execute_input":"2025-07-17T22:44:52.850870Z","iopub.status.idle":"2025-07-17T22:44:53.136693Z","shell.execute_reply.started":"2025-07-17T22:44:52.850847Z","shell.execute_reply":"2025-07-17T22:44:53.135709Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# PEFT config\npeft_config = LoraConfig(\n   lora_alpha=lora_alpha,\n   lora_dropout=lora_dropout,\n   r=lora_r,\n   bias=\"none\",\n   task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:53.138272Z","iopub.execute_input":"2025-07-17T22:44:53.138734Z","iopub.status.idle":"2025-07-17T22:44:53.143460Z","shell.execute_reply.started":"2025-07-17T22:44:53.138695Z","shell.execute_reply":"2025-07-17T22:44:53.142569Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Training arguments\ntraining_arguments = TrainingArguments(\n   output_dir=output_dir,\n   num_train_epochs=num_train_epochs,\n   per_device_train_batch_size=per_device_train_batch_size,\n   gradient_accumulation_steps=gradient_accumulation_steps,\n   optim=optim,\n   save_steps=save_steps,\n   logging_steps=logging_steps,\n   learning_rate=learning_rate,\n   weight_decay=weight_decay,\n   fp16=fp16,\n   bf16=bf16,\n   max_grad_norm=max_grad_norm,\n   max_steps=max_steps,\n   warmup_ratio=warmup_ratio,\n   group_by_length=group_by_length,\n   lr_scheduler_type=lr_scheduler_type,\n   report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:53.144482Z","iopub.execute_input":"2025-07-17T22:44:53.144729Z","iopub.status.idle":"2025-07-17T22:44:53.176869Z","shell.execute_reply.started":"2025-07-17T22:44:53.144703Z","shell.execute_reply":"2025-07-17T22:44:53.176208Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Create trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:53.177647Z","iopub.execute_input":"2025-07-17T22:44:53.177902Z","iopub.status.idle":"2025-07-17T22:44:53.706443Z","shell.execute_reply.started":"2025-07-17T22:44:53.177882Z","shell.execute_reply":"2025-07-17T22:44:53.705566Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Before training\nprompt = \"What is a large language model?\"\ninstruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n\npretrain_pipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=128\n)\n\npretrain_result = pretrain_pipe(instruction)\nprint(\"Before fine-tuning:\", pretrain_result[0]['generated_text'][len(instruction):])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:53.707744Z","iopub.execute_input":"2025-07-17T22:44:53.708099Z","iopub.status.idle":"2025-07-17T22:44:55.363377Z","shell.execute_reply.started":"2025-07-17T22:44:53.708068Z","shell.execute_reply":"2025-07-17T22:44:55.362477Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Before fine-tuning: A large language model is a neural network that has been trained on a large corpus of text data. It is capable of generating human-like text based on the training data.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:55.364299Z","iopub.execute_input":"2025-07-17T22:44:55.364540Z","iopub.status.idle":"2025-07-17T22:44:58.460549Z","shell.execute_reply.started":"2025-07-17T22:44:55.364520Z","shell.execute_reply":"2025-07-17T22:44:58.459339Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=2.270804595947266, metrics={'train_runtime': 2.5267, 'train_samples_per_second': 3.958, 'train_steps_per_second': 3.958, 'total_flos': 5578677080064.0, 'train_loss': 2.270804595947266})"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# Save model\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:58.461897Z","iopub.execute_input":"2025-07-17T22:44:58.462280Z","iopub.status.idle":"2025-07-17T22:44:58.752292Z","shell.execute_reply.started":"2025-07-17T22:44:58.462248Z","shell.execute_reply":"2025-07-17T22:44:58.750401Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# After training\nprompt = \"What is a large language model?\"\ninstruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\nresult = pipe(instruction)\nprint(result[0]['generated_text'][len(instruction):])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:44:58.753939Z","iopub.execute_input":"2025-07-17T22:44:58.754399Z","iopub.status.idle":"2025-07-17T22:45:03.432679Z","shell.execute_reply.started":"2025-07-17T22:44:58.754354Z","shell.execute_reply":"2025-07-17T22:45:03.431773Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"A large language model is a type of artificial intelligence model used for natural language processing (NLP) in speech recognition, text classification, and language understanding. It is a complex computer model that has been trained using billions or trillions of examples of natural language data. Large language models are capable of processing large amounts of data and making accurate predictions in real-time. They are essential for advancing NLP technologies and enabling machines to understand natural language without the need for manually labelled training data.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"from IPython.display import display, Markdown\nimport pandas as pd\n\ndef clean(text, max_len=300):\n    return text.replace(\"|\", \"Â¦\").replace(\"\\n\", \" \").strip()[:max_len] + \"...\"\n\nprompts = [\n    \"What is a large language model?\",\n    \"Explain what machine learning is.\",\n    \"What's the capital of Germany? What is there to see in the capital of Germany?\",\n    \"How do airplanes fly?\",\n    \"Explain photosynthesis in a simple way\",\n    \"Describe a sustainable city of the future, including transport, energy, and social systems.\",\n    \"Generate a short dialog between a doctor and a patient concerned about climate change.\"\n]\n\nresults = []\nfor prompt in prompts:\n    instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n    before = pretrain_pipe(instruction, max_length=10000)[0]['generated_text'][len(instruction):].strip()\n    after = pipe(instruction, max_length=10000)[0]['generated_text'][len(instruction):].strip()\n    results.append({\n        \"Prompt\": prompt,\n        \"Before Fine-Tuning\": clean(before),\n        \"After Fine-Tuning\": clean(after)\n    })\n\ndf = pd.DataFrame(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T22:56:58.747565Z","iopub.execute_input":"2025-07-17T22:56:58.748050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the comparison table\ndef make_comparison_view(df_subset):\n    md = \"## Prompt vs Output Comparison\\n\\n\\n\"\n    for row in df_subset.itertuples():\n        md += f\"<details>\\n<summary><strong>{row.Prompt}</strong></summary>\\n\\n\"\n        md += f\"**Before Fine-Tuning:**\\n\\n```\\n{row._2.strip()}\\n```\\n\\n\"\n        md += f\"**After Fine-Tuning:**\\n\\n```\\n{row._3.strip()}\\n```\\n\"\n        md += \"</details>\\n\\n\"\n    return md\n\ndisplay(Markdown(make_comparison_view(df)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}