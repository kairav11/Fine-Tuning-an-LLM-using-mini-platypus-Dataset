# Fine-Tuning-an-LLM-using-mini-platypus-Dataset
This project demonstrates the process of fine-tuning a pre-trained Large Language Model (LLM) using QLoRA, and provides a simple interface to compare model responses before and after fine-tuning. It is built and tested in a Kaggle notebook environment using Hugging Face Transformers and  TRL libraries.
